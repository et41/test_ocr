{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrOCR Fine-tuning — Transformer Test Reports\n",
    "\n",
    "Fine-tunes `microsoft/trocr-base-handwritten` on labeled crop images from transformer test reports.\n",
    "\n",
    "**Before running:**\n",
    "1. Runtime → Change runtime type → **T4 GPU**\n",
    "2. Upload your `data/labels.csv` and `data/crops/` folder to Google Drive\n",
    "3. Update `DRIVE_DATA_PATH` in the Config cell below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Go to Runtime → Change runtime type → T4 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"Drive mounted at /content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Config — Edit paths here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# --- EDIT THESE ---\n",
    "# Folder in your Google Drive that contains:\n",
    "#   labels.csv\n",
    "#   crops/   (subfolder with all .png crop images)\n",
    "DRIVE_DATA_PATH = Path(\"/content/drive/MyDrive/text_recognizer_data\")\n",
    "\n",
    "# Where to save the fine-tuned model on Drive\n",
    "DRIVE_MODEL_OUTPUT = Path(\"/content/drive/MyDrive/text_recognizer_model/trocr_finetuned\")\n",
    "\n",
    "# Training hyperparameters\n",
    "EPOCHS    = 30\n",
    "BATCH_SIZE = 8\n",
    "LR        = 5e-5\n",
    "PATIENCE  = 8\n",
    "VAL_SPLIT = 0.2\n",
    "# ------------------\n",
    "\n",
    "LABELS_CSV   = DRIVE_DATA_PATH / \"labels.csv\"\n",
    "CROPS_DIR    = DRIVE_DATA_PATH / \"crops\"\n",
    "CHECKPOINT   = Path(\"/content/trocr_finetuned\")\n",
    "\n",
    "print(f\"Labels CSV : {LABELS_CSV}\")\n",
    "print(f\"Crops dir  : {CROPS_DIR}\")\n",
    "print(f\"Output     : {DRIVE_MODEL_OUTPUT}\")\n",
    "\n",
    "assert LABELS_CSV.exists(), f\"labels.csv not found at {LABELS_CSV}\"\n",
    "assert CROPS_DIR.exists(),  f\"crops/ folder not found at {CROPS_DIR}\"\n",
    "print(\"All paths OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers sentencepiece opencv-contrib-python-headless Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clone your repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "REPO_DIR = Path(\"/content/text_recognizer\")\n",
    "\n",
    "if REPO_DIR.exists():\n",
    "    !git -C {REPO_DIR} pull\n",
    "else:\n",
    "    !git clone https://github.com/et41/test_ocr.git {REPO_DIR}\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Preview training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "samples = []\n",
    "with open(LABELS_CSV, newline=\"\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        value = row[\"value\"].strip()\n",
    "        if value:\n",
    "            # Remap image_path to Drive location\n",
    "            img_name = Path(row[\"image_path\"]).name\n",
    "            img_path = str(CROPS_DIR / img_name)\n",
    "            samples.append((img_path, value))\n",
    "\n",
    "print(f\"Total labeled samples: {len(samples)}\")\n",
    "print(\"\\nFirst 10 samples:\")\n",
    "for path, val in samples[:10]:\n",
    "    exists = \"OK\" if Path(path).exists() else \"MISSING\"\n",
    "    print(f\"  [{exists}] {Path(path).name} → '{val}'\")\n",
    "\n",
    "missing = sum(1 for p, _ in samples if not Path(p).exists())\n",
    "if missing:\n",
    "    print(f\"\\nWARNING: {missing} image(s) not found in {CROPS_DIR}\")\n",
    "else:\n",
    "    print(f\"\\nAll {len(samples)} images found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import csv\nimport random\nimport sys\nfrom pathlib import Path\n\nimport cv2\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel\n\n# Always re-resolve the repo path so this cell works even after a kernel restart\nREPO_DIR = Path(\"/content/text_recognizer\")\nif str(REPO_DIR) not in sys.path:\n    sys.path.insert(0, str(REPO_DIR))\n\nfrom pipeline.dataset import augment_image\n\nALLOWED_CHARS = set(\"0123456789.,-+\")\n\n\nclass TrOCRDataset(Dataset):\n    def __init__(self, samples, processor, augment=False):\n        self.samples  = samples\n        self.processor = processor\n        self.augment  = augment\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        image_path, value = self.samples[idx]\n        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n        if image is None:\n            image = np.zeros((32, 64), dtype=np.uint8)\n        if self.augment:\n            image = augment_image(image)\n        image_rgb  = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n        pil_img    = Image.fromarray(image_rgb)\n        pixel_vals = self.processor(images=pil_img, return_tensors=\"pt\").pixel_values.squeeze(0)\n        labels = self.processor.tokenizer(\n            text=value, return_tensors=\"pt\",\n            padding=\"max_length\", max_length=20, truncation=True,\n        ).input_ids.squeeze(0)\n        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n        return {\"pixel_values\": pixel_vals, \"labels\": labels, \"text\": value}\n\n\nprint(\"Dataset class defined.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load base TrOCR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TROCR_BASE = \"microsoft/trocr-base-handwritten\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "print(f\"\\nDownloading {TROCR_BASE} (~400 MB, only on first run)...\")\n",
    "processor = TrOCRProcessor.from_pretrained(TROCR_BASE)\n",
    "model     = VisionEncoderDecoderModel.from_pretrained(TROCR_BASE)\n",
    "\n",
    "model.config.decoder_start_token_id = processor.tokenizer.bos_token_id\n",
    "model.config.pad_token_id           = processor.tokenizer.pad_token_id\n",
    "model.config.eos_token_id           = processor.tokenizer.eos_token_id\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Build train / val splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(samples)\n",
    "val_size     = max(1, int(len(samples) * VAL_SPLIT))\n",
    "val_samples  = samples[:val_size]\n",
    "train_samples = samples[val_size:]\n",
    "\n",
    "print(f\"Train: {len(train_samples)} samples\")\n",
    "print(f\"Val  : {len(val_samples)} samples\")\n",
    "\n",
    "train_set = TrOCRDataset(train_samples, processor, augment=True)\n",
    "val_set   = TrOCRDataset(val_samples,   processor, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_set,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "def compute_accuracy(preds, targets):\n",
    "    correct = sum(\n",
    "        \"\".join(c for c in p.strip() if c in ALLOWED_CHARS) == t\n",
    "        for p, t in zip(preds, targets)\n",
    "    )\n",
    "    return correct / max(len(targets), 1)\n",
    "\n",
    "\n",
    "best_val_loss  = float(\"inf\")\n",
    "epochs_no_impr = 0\n",
    "history        = []\n",
    "\n",
    "CHECKPOINT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Starting fine-tuning for up to {EPOCHS} epochs...\\n\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # --- Train ---\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        pv     = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        loss   = model(pixel_values=pv, labels=labels).loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # --- Validate ---\n",
    "    model.eval()\n",
    "    val_loss   = 0.0\n",
    "    all_preds  = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            pv     = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            val_loss += model(pixel_values=pv, labels=labels).loss.item()\n",
    "            gen_ids  = model.generate(pv, max_new_tokens=20)\n",
    "            preds    = processor.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(batch[\"text\"])\n",
    "    val_loss /= len(val_loader)\n",
    "    acc = compute_accuracy(all_preds, all_targets)\n",
    "\n",
    "    history.append({\"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss, \"acc\": acc})\n",
    "    print(f\"Epoch {epoch:3d} | Train: {train_loss:.4f} | Val: {val_loss:.4f} | Acc: {acc:.2%}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss  = val_loss\n",
    "        epochs_no_impr = 0\n",
    "        model.save_pretrained(str(CHECKPOINT))\n",
    "        processor.save_pretrained(str(CHECKPOINT))\n",
    "        print(f\"  -> Saved best model (val loss {best_val_loss:.4f})\")\n",
    "    else:\n",
    "        epochs_no_impr += 1\n",
    "        if epochs_no_impr >= PATIENCE:\n",
    "            print(f\"\\nEarly stopping after {PATIENCE} epochs without improvement.\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nTraining complete. Best val loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Plot training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs_x   = [h[\"epoch\"]      for h in history]\n",
    "train_loss = [h[\"train_loss\"] for h in history]\n",
    "val_loss   = [h[\"val_loss\"]   for h in history]\n",
    "acc        = [h[\"acc\"]        for h in history]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(epochs_x, train_loss, label=\"Train loss\")\n",
    "ax1.plot(epochs_x, val_loss,   label=\"Val loss\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"Loss curves\")\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(epochs_x, [a * 100 for a in acc], color=\"green\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Accuracy (%)\")\n",
    "ax2.set_title(\"Validation accuracy\")\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/content/training_curves.png\", dpi=120)\n",
    "plt.show()\n",
    "print(\"Saved training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Quick inference test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a few val samples\n",
    "model.eval()\n",
    "print(f\"{'Image':<40} {'Ground truth':<15} {'Predicted':<15} {'Match'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for img_path, gt in val_samples[:10]:\n",
    "    image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        continue\n",
    "    image_rgb  = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    pil_img    = Image.fromarray(image_rgb)\n",
    "    pixel_vals = processor(images=pil_img, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen_ids = model.generate(pixel_vals, max_new_tokens=20)\n",
    "    pred = processor.batch_decode(gen_ids, skip_special_tokens=True)[0].strip()\n",
    "    pred = \"\".join(c for c in pred if c in ALLOWED_CHARS)\n",
    "\n",
    "    match = \"OK\" if pred == gt else \"FAIL\"\n",
    "    print(f\"{Path(img_path).name:<40} {gt:<15} {pred:<15} {match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save model to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "DRIVE_MODEL_OUTPUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy checkpoint to Drive\n",
    "for f in CHECKPOINT.iterdir():\n",
    "    shutil.copy2(f, DRIVE_MODEL_OUTPUT / f.name)\n",
    "\n",
    "# Also copy training curves\n",
    "shutil.copy2(\"/content/training_curves.png\", DRIVE_MODEL_OUTPUT.parent / \"training_curves.png\")\n",
    "\n",
    "print(f\"Model saved to Google Drive: {DRIVE_MODEL_OUTPUT}\")\n",
    "print(\"\\nFiles saved:\")\n",
    "for f in sorted(DRIVE_MODEL_OUTPUT.iterdir()):\n",
    "    size_mb = f.stat().st_size / 1e6\n",
    "    print(f\"  {f.name:<35} {size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Download model to your PC (alternative to Drive)\n",
    "\n",
    "Run this cell if you prefer to download the model directly instead of using Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# Zip the checkpoint\n",
    "zip_path = \"/content/trocr_finetuned.zip\"\n",
    "shutil.make_archive(\"/content/trocr_finetuned\", \"zip\", str(CHECKPOINT))\n",
    "print(f\"Zipped model: {zip_path}\")\n",
    "\n",
    "# Trigger browser download\n",
    "files.download(zip_path)"
   ]
  }
 ]
}